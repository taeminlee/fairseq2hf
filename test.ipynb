{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fairseq \n",
    "## 체크포인트 경로\n",
    "/data/koo/datasets/bts/AIHUB/clean/AIHUB_all_/ckpt/checkpoint_best.pt\n",
    "\n",
    "##bin 파일 경로 \n",
    "/data/koo/datasets/bts/AIHUB/clean/bin-1.0\n",
    "\n",
    "# huggingface fsmt 모델\n",
    "https://huggingface.co/facebook/wmt19-en-de\n",
    "\n",
    "# 기존 gen 파라미터\n",
    "python fairseq/fairseq_cli/generate.py datasets/cyd_dataset/bin-cyd  \\\n",
    "--path ./cyd-ckpt/ckpt/checkpoint_best.pt \\\n",
    "--task translation \\\n",
    "--source-lang src --target-lang tgt \\\n",
    "--gen-subset test \\\n",
    "--empty-cache-freq 1000 \\\n",
    "--batch-size 150 \\\n",
    "--remove-bpe sentencepiece > results.txt\n",
    "\n",
    "\n",
    "# train 인자\n",
    "GPUNUM=0\n",
    "DIRNAME=./datasets/cyd_dataset/bin-cyd\n",
    "FILENAME=./cyd-ckpt\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=${GPUNUM} nohup fairseq-train \\\n",
    "${DIRNAME} \\\n",
    "—fp16 \\\n",
    "—fp16-init-scale 4096 \\\n",
    "—arch transformer —share-decoder-input-output-embed \\\n",
    "—optimizer adam —adam-betas '(0.9, 0.98)' —clip-norm 0.0 \\\n",
    "—lr 5e-4 —lr-scheduler inverse_sqrt —warmup-updates 4000 \\\n",
    "—dropout 0.3 —weight-decay 0.0001 \\\n",
    "—task translation \\\n",
    "—criterion label_smoothed_cross_entropy —label-smoothing 0.1 \\\n",
    "—max-tokens 4096 \\\n",
    "—update-freq 4 \\\n",
    "—eval-bleu \\\n",
    "—eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "—eval-bleu-detok moses \\\n",
    "—eval-bleu-remove-bpe \\\n",
    "—eval-bleu-print-samples \\\n",
    "—best-checkpoint-metric bleu —maximize-best-checkpoint-metric \\\n",
    "—tensorboard-logdir tblog \\\n",
    "—log-interval 100 \\\n",
    "—max-update 200000 \\\n",
    "—skip-invalid-size-inputs-valid-test \\\n",
    "—no-epoch-checkpoints \\\n",
    "—save-dir ${FILENAME}/ckpt  \\\n",
    "—share-all-embeddings &\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /mnt/raid6/persuade/miniconda3/envs/117/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /mnt/raid6/persuade/miniconda3/envs/117/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /mnt/raid6/persuade/miniconda3/envs/117/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/raid6/persuade/miniconda3/envs/117/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/mnt/raid6/persuade/miniconda3/envs/117/lib/libcudart.so'), PosixPath('/mnt/raid6/persuade/miniconda3/envs/117/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 1.08G/1.08G [02:58<00:00, 6.06MB/s]\n",
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-en-de and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading generation_config.json: 100%|██████████| 235/235 [00:00<00:00, 1.45MB/s]\n"
     ]
    }
   ],
   "source": [
    "mname = \"facebook/wmt19-en-de\"\n",
    "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "model = FSMTForConditionalGeneration.from_pretrained(mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers.convert_slow_tokenizer import SentencePieceExtractor\n",
    "\n",
    "vocab_scores = {}\n",
    "with open('/data/koo/datasets/bts/AIHUB/clean/spm.vocab', 'r') as fp:\n",
    "    for line in fp.readlines():\n",
    "        if len(line.split('\\t')) != 2:\n",
    "            raise Exception(f'error while reading spm.vocab! {line}')\n",
    "        vocab_scores[line.split('\\t')[0]] = float(line.split('\\t')[1])\n",
    "\n",
    "vocab, merges = SentencePieceExtractor('/data/koo/datasets/bts/AIHUB/clean/spm.model').extract(vocab_scores)\n",
    "\n",
    "with open('merges.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join([' '.join(pair) for pair in merges]))\n",
    "with open('vocab-tgt.json', 'w') as fp:\n",
    "    json.dump(vocab, fp, ensure_ascii=False, indent=4)\n",
    "with open('vocab-src.json', 'w') as fp:\n",
    "    json.dump(vocab, fp, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
